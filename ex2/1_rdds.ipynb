{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/06/01 18:28:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "23/06/01 18:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "23/06/01 18:28:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Create a custom Spark config to maximize performance:\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.driver.memory\", \"4g\")\n",
    "    .set(\"spark.executor.memory\", \"7392m\")\n",
    "    .set(\"spark.parallelism\", \"4\")\n",
    ")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ChiSquaredRDD\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reviews as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 ms, sys: 978 µs, total: 3.06 ms\n",
      "Wall time: 564 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "input_rdd = sc.textFile(review_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 ms, sys: 0 ns, total: 1.86 ms\n",
      "Wall time: 1.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load stopwords into (local) memory (Note: file contains duplicates, so convert to set)\n",
    "stopwords = set(open(\"stopwords.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: local variables like this one will be automatically broadcast to all data nodes if accessed in any RDD transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the total number of documents and number of documents per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we can just count the number of documents by category and then sum up the number of documents per category to get the total number of documents.\n",
    "\n",
    "We create an RDD for the category tag of each review (parsing the input JSON string and extracting the `category` attribute) and then compute the counts by calling `countByValue()` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 835 µs, sys: 48 µs, total: 883 µs\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "category_rdd = input_rdd \\\n",
    "    .map(lambda input_string: json.loads(input_string)['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 ms, sys: 4.84 ms, total: 13.2 ms\n",
      "Wall time: 3.55 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Patio_Lawn_and_Garde': 994,\n",
       "             'Apps_for_Android': 2638,\n",
       "             'Book': 22507,\n",
       "             'Toys_and_Game': 2253,\n",
       "             'Office_Product': 1243,\n",
       "             'Digital_Music': 836,\n",
       "             'Sports_and_Outdoor': 3269,\n",
       "             'Automotive': 1374,\n",
       "             'Beauty': 2023,\n",
       "             'Musical_Instrument': 500,\n",
       "             'CDs_and_Vinyl': 3749,\n",
       "             'Kindle_Store': 3205,\n",
       "             'Clothing_Shoes_and_Jewelry': 5749,\n",
       "             'Electronic': 7825,\n",
       "             'Home_and_Kitche': 4254,\n",
       "             'Cell_Phones_and_Accessorie': 3447,\n",
       "             'Pet_Supplie': 1235,\n",
       "             'Movies_and_TV': 4607,\n",
       "             'Baby': 916,\n",
       "             'Tools_and_Home_Improvement': 1926,\n",
       "             'Grocery_and_Gourmet_Food': 1297,\n",
       "             'Health_and_Personal_Care': 2982})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "category_counts = category_rdd.countByValue()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dict is minuscule (one key-value pair for each category) and will easily fit into memory on the data-nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 µs, sys: 0 ns, total: 22 µs\n",
      "Wall time: 25.5 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "review_count = sum(category_counts.values())\n",
    "review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the number of occasions of each term by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define an RDD for extracting `category` and `reviewText` from each review JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 µs, sys: 355 µs, total: 679 µs\n",
      "Wall time: 579 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "category_review_rdd = input_rdd \\\n",
    "    .map(lambda json_str: json.loads(json_str)) \\\n",
    "    .map(lambda json_obj: (json_obj['category'], json_obj['reviewText']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tokenize the review texts and remove stopwords to obtain the terms. For each unique term appearing in each document, output a tuple of the form $$((term, category), 1)$$ where $term$ is the respective term and $category$ is the category the document is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 671 µs, total: 13.3 ms\n",
      "Wall time: 70.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define pattern for splitting/tokenizing\n",
    "pattern = re.compile(r\"[^a-zA-Z<>^|]+\")\n",
    "\n",
    "\n",
    "def map_review_data(pair):\n",
    "    category, review_text = pair\n",
    "    # obtain set of unique(!) terms for each document via tokenization followed by stopword removal\n",
    "    terms = [\n",
    "        t for t in set(token.lower() for token in pattern.split(review_text)) if t not in stopwords and len(t) >= 2\n",
    "    ]\n",
    "    return [((term, category), 1) for term in terms]\n",
    "\n",
    "\n",
    "def remap(pair):\n",
    "    term_and_cat, count = pair\n",
    "    term, cat = term_and_cat\n",
    "    return term, (cat, count)\n",
    "\n",
    "\n",
    "term_cat_occ_rdd = category_review_rdd \\\n",
    "    .flatMap(map_review_data) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(remap) \\\n",
    "    .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data to compute the $\\chi^2$ metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_chi_square(pair):\n",
    "    term, term_counts_for_categories = pair\n",
    "\n",
    "    # Use to retrieve number of documents containing term for a particular category\n",
    "    doc_count_for_cat = dict(term_counts_for_categories)\n",
    "\n",
    "    # total number of documents containing the term\n",
    "    total_doc_count_for_term = sum(doc_count_for_cat.values())\n",
    "\n",
    "    term_and_cat_chi_squared = []\n",
    "\n",
    "    for category, count in doc_count_for_cat.items():\n",
    "        # number of documents in c which contain t\n",
    "        a = count\n",
    "        # number of documents not in c which contain t\n",
    "        b = total_doc_count_for_term - a\n",
    "        # number of documents in c without t\n",
    "        c = category_counts[category] - a\n",
    "        # number of documents not in c without t\n",
    "        d = review_count - a - b - c\n",
    "        term_and_cat_chi_squared.append(\n",
    "            (\n",
    "                category,\n",
    "                (term, review_count * (a * d - b * c) ** 2 / ((a + b) * (a + c) * (b + d) * (c + d)))\n",
    "            )\n",
    "        )\n",
    "    return term_and_cat_chi_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 1.11 ms, total: 6.11 ms\n",
      "Wall time: 25.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the chi-squared value for each unique term and category pair\n",
    "# (term, category) -> chi-square\n",
    "term_cat_chi_squared_rdd = term_cat_occ_rdd \\\n",
    "    .flatMap(calculate_chi_square) \\\n",
    "    .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the top 75 terms for each category (sorted by $\\chi^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 ms, sys: 4.15 ms, total: 30.4 ms\n",
      "Wall time: 7.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Perform a top K query for each category\n",
    "topK = 75  # Number of terms to retrieve per category\n",
    "\n",
    "\n",
    "def get_top_terms(pair):\n",
    "    category, terms = pair\n",
    "    top_terms = sorted(terms, key=lambda x: x[1], reverse=True)[:topK]  # Sort and retrieve top K terms\n",
    "    return category, top_terms\n",
    "\n",
    "\n",
    "# Apply the transformation and collect the results\n",
    "results = term_cat_chi_squared_rdd.map(get_top_terms).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain all unique tokens from Top 75 for every category (ordered alphabetically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 765 µs, sys: 0 ns, total: 765 µs\n",
      "Wall time: 779 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens = sorted(set(term for _, top75_for_cat in results for term, _ in top75_for_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Format of each output line: \"<category> term1:chi_squared1 term2:chi_squared2 ... term75:chi_squared75\"\n",
    "# finally, append the list of tokens to the end of the file\n",
    "with open(f\"output_rdd.txt\", \"w\") as file:\n",
    "    for pair in results:\n",
    "        file.write(\"<%s>\" % pair[0] + \" \")\n",
    "        for token, chi_square in pair[1]:\n",
    "            file.write(\"%s:%f\" % (token, chi_square) + \" \")\n",
    "        file.write(\"\\n\")\n",
    "    file.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()  # stop Spark context to free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
