{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/05/25 10:04:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/05/25 10:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/05/25 10:04:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ChiSquaredRDD\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reviews as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "# review_path = \"hdfs:///user/e11809642/reviews/reduced_devset.json\"\n",
    "input_rdd = sc.textFile(review_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stopwords into (local) memory (Note: file contains duplicates, so convert to set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords_path = 'stopwords.txt'\n",
    "\n",
    "def load_unique_lines(filename):\n",
    "    lines = set()\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace and newline characters\n",
    "            lines.add(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "stopwords_local = load_unique_lines(stopwords_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast stopword data 'into Spark' so that it can be used in other RDDs (broadcasting should be fine as data easily fits into memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = sc.broadcast(stopwords_local)\n",
    "# values stored in broadcast variables are accessible via value prop (for this case use stopwords.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse JSON strings, extract the category + review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:34:45.585066Z",
     "start_time": "2023-04-29T18:34:45.573062Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "category_review_rdd = input_rdd \\\n",
    "    .map(lambda json_str: json.loads(json_str)) \\\n",
    "    .map(lambda json_obj: (json_obj['category'], json_obj['reviewText']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:34:47.233787Z",
     "start_time": "2023-04-29T18:34:45.585066Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review_count = category_review_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute number of documents per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define RDD with required transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_counts_rdd = category_review_rdd \\\n",
    "    .map(lambda pair: (pair[0], 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, collect the values of the RDD into a local list. This list is really small and will easily fit into memory on the datanodes. Apparently, when using it within a `map()` or other transformations applied on an RDD, the data will automatically be broadcast across all data nodes with as many values as there are categories and therefore really small) and broadcast them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_counts = category_counts_rdd.collect()\n",
    "\n",
    "# alternative approach - for explanation of difference see: https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables\n",
    "# create broadcast variable\n",
    "#category_counts = sc.broadcast(category_counts_rdd.collect())\n",
    "# use .value to access the value that is now broadcast across all data nodes\n",
    "#category_counts.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain terms via tokenization followed by stopword removal, compute number of occurrences of each term in each category across all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define pattern for splitting/tokenizing\n",
    "import re\n",
    "pattern = re.compile(r\"[^a-zA-Z<>^|]+\")\n",
    "\n",
    "# obtain terms and occurrences per category\n",
    "term_category_occurrences_rdd = category_review_rdd \\\n",
    "    .map(lambda pair: ((pair[0], pair[1]), set(pattern.split(pair[1])))) \\\n",
    "    .flatMap(lambda pair: (((term.lower(), pair[0][0]), 1) for term in pair[1] if\n",
    "                           term.lower() not in stopwords_local and len(term) >= 2)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute the number of occurrences of each term across all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_occurrences_rdd = term_category_occurrences_rdd \\\n",
    "    .map(lambda pair: (pair[0][0], pair[1])) \\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scripture', 119), ('verse', 73), ('tremendously', 58)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_occurrences_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('mic', 'Musical_Instrument'), 25),\n",
       " (('setups', 'Musical_Instrument'), 1),\n",
       " (('tape', 'Musical_Instrument'), 5)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_category_occurrences_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('colada', 'Health_and_Personal_Care'), 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_occurrences_rdd.union(term_category_occurrences).collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:53:14.481513Z",
     "start_time": "2023-04-29T18:53:14.460034Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_chi_square(term, category):\n",
    "    \"\"\"\n",
    "    Computes the chi-squared value for a given term and category\n",
    "    \"\"\"\n",
    "    a = term_category_occurrences_rdd[(term, category)]\n",
    "    b = term_occurrences_rdd[term] - a\n",
    "    c = category_counts_rdd[category] - a\n",
    "    d = review_count - a - b - c\n",
    "    return review_count * (a * d - b * c) ** 2 / ((a + b) * (a + c) * (b + d) * (c + d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mterm_category_occurrences_rdd\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMusical_Instrument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "term_category_occurrences_rdd[('mic', 'Musical_Instrument')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scripture', 119),\n",
       " ('verse', 73),\n",
       " ('tremendously', 58),\n",
       " ('chapter', 887),\n",
       " ('friendly', 310)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_occurrences_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('mic', 'Musical_Instrument'), 25),\n",
       " (('setups', 'Musical_Instrument'), 1),\n",
       " (('tape', 'Musical_Instrument'), 5),\n",
       " (('clip', 'Musical_Instrument'), 5),\n",
       " (('altering', 'Musical_Instrument'), 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_category_occurrences.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272560"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term_category_occurrences.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('horizons', 'Patio_Lawn_and_Garde'), 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_category_occurrences.take(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/usr/lib/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/lib/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 262, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:563\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:262\u001b[0m, in \u001b[0;36mRDD.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle an RDD, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt appears that you are attempting to broadcast an RDD or reference an RDD from an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction or transformation. RDD transformations and actions can only be invoked by the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver, not inside of other transformations; for example, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformation and count action cannot be performed inside of the rdd1.map \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformation. For more information, see SPARK-5063.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the chi-squared value for each unique term and category pair\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (term, category) -> chi-square\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m term_category_chi_squared_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mterm_category_occurrences_rdd\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_chi_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:2240\u001b[0m, in \u001b[0;36mRDD.groupByKey\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merger\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2239\u001b[0m locally_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(combine, preservesPartitioning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2240\u001b[0m shuffled \u001b[38;5;241m=\u001b[39m \u001b[43mlocally_combined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgroupByKey\u001b[39m(it):\n\u001b[1;32m   2243\u001b[0m     merger \u001b[38;5;241m=\u001b[39m ExternalGroupBy(agg, memory, serializer)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:2083\u001b[0m, in \u001b[0;36mRDD.partitionBy\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2080\u001b[0m keyed\u001b[38;5;241m.\u001b[39m_bypass_serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[1;32m   2082\u001b[0m     pairRDD \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPairwiseRDD(\n\u001b[0;32m-> 2083\u001b[0m         \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241m.\u001b[39mrdd())\u001b[38;5;241m.\u001b[39masJavaPairRDD()\n\u001b[1;32m   2084\u001b[0m     jpartitioner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonPartitioner(numPartitions,\n\u001b[1;32m   2085\u001b[0m                                                    \u001b[38;5;28mid\u001b[39m(partitionFunc))\n\u001b[1;32m   2086\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mvalueOfPair(pairRDD\u001b[38;5;241m.\u001b[39mpartitionBy(jpartitioner))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:2951\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2949\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2951\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[1;32m   2954\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:2830\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2829\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 2830\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[1;32m   2832\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:2816\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 2816\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "# Compute the chi-squared value for each unique term and category pair\n",
    "# (term, category) -> chi-square\n",
    "term_category_chi_squared_rdd = term_category_occurrences_rdd \\\n",
    "    .map(lambda pair: (pair[0][0], (pair[0][1], calculate_chi_square(pair[0][0], pair[0][1])))) \\\n",
    "    .groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'term_category_chi_squared_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select the top 75 tokens with the highest chi-square value for each category\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (category) -> [(token, chi-square)]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m chi_square_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mterm_category_chi_squared_rdd\u001b[49m \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m pair: (pair[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28msorted\u001b[39m(pair[\u001b[38;5;241m1\u001b[39m], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m75\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'term_category_chi_squared_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the top 75 tokens with the highest chi-square value for each category\n",
    "# (category) -> [(token, chi-square)]\n",
    "chi_square_rdd = term_category_chi_squared_rdd \\\n",
    "    .map(lambda pair: (pair[0], sorted(pair[1], key=lambda x: x[1], reverse=True)[:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all unique tokens from the top 75 tokens with the highest chi-square value for each category\n",
    "tokens = chi_square_rdd \\\n",
    "    .flatMap(lambda pair: (token for token, chi_square in pair[1])) \\\n",
    "    .distinct() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the tokens in alphabetical order\n",
    "tokens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_rdd = chi_square_rdd.sortByKey()\n",
    "\n",
    "# Save the top 75 tokens with the highest chi-square value for each category to a file in the local file system\n",
    "# in the format: \"<category> term1:chi_squared1 term2:chi_squared2 ... term75:chi_squared75\" for each line and append the list of tokens to the end of the file\n",
    "with open(\"chi_squared.txt\", \"a\") as file:\n",
    "    for pair in chi_square_rdd.collect():\n",
    "        file.write(\"<%s>\" % pair[0] + \" \")\n",
    "        for token, chi_square in pair[1]:\n",
    "            file.write(\"%s:%f\" % (token, chi_square) + \" \")\n",
    "        file.write(\"\\n\")\n",
    "    file.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:34:23.445308Z",
     "start_time": "2023-04-29T18:34:22.817267Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
