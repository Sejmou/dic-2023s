{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/06/01 12:24:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/06/01 12:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/06/01 12:24:46 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Create a custom Spark config to maximize performance:\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.driver.memory\", \"4g\")\n",
    "    .set(\"spark.executor.memory\", \"7392m\")\n",
    "    .set(\"spark.parallelism\", \"4\")\n",
    ")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ChiSquaredRDD\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define helper function for benchmarking steps of Chi-square computation 'pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reviews as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 ms, sys: 1.39 ms, total: 2.83 ms\n",
      "Wall time: 654 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "# review_path = \"hdfs:///user/e11809642/reviews/reduced_devset.json\"\n",
    "input_rdd = sc.textFile(review_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stopwords into (local) memory (Note: file contains duplicates, so convert to set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 700 µs, total: 700 µs\n",
      "Wall time: 603 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stopwords_path = 'stopwords.txt'\n",
    "\n",
    "def load_unique_lines(filename):\n",
    "    lines = set()\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace and newline characters\n",
    "            lines.add(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "stopwords = load_unique_lines(stopwords_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: local variables like this one will be automatically broadcast to all data nodes if accessed in any RDD transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute total number of documents and number of documents per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we can just count the number of documents by category and then sum up the number of documents per category to get the total number of documents.\n",
    "\n",
    "We create an RDD for the category tag of each review (parsing the input JSON string and extracting the `category` attribute) and then compute the counts by calling `countByValue()` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "category_rdd = (\n",
    "    input_rdd\n",
    "    .map(lambda inputstring: json.loads((inputstring))['category'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.99 ms, sys: 6.15 ms, total: 15.1 ms\n",
      "Wall time: 3.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Patio_Lawn_and_Garde': 994,\n",
       "             'Apps_for_Android': 2638,\n",
       "             'Book': 22507,\n",
       "             'Toys_and_Game': 2253,\n",
       "             'Office_Product': 1243,\n",
       "             'Digital_Music': 836,\n",
       "             'Sports_and_Outdoor': 3269,\n",
       "             'Automotive': 1374,\n",
       "             'Beauty': 2023,\n",
       "             'Musical_Instrument': 500,\n",
       "             'CDs_and_Vinyl': 3749,\n",
       "             'Kindle_Store': 3205,\n",
       "             'Clothing_Shoes_and_Jewelry': 5749,\n",
       "             'Electronic': 7825,\n",
       "             'Home_and_Kitche': 4254,\n",
       "             'Cell_Phones_and_Accessorie': 3447,\n",
       "             'Pet_Supplie': 1235,\n",
       "             'Movies_and_TV': 4607,\n",
       "             'Baby': 916,\n",
       "             'Tools_and_Home_Improvement': 1926,\n",
       "             'Grocery_and_Gourmet_Food': 1297,\n",
       "             'Health_and_Personal_Care': 2982})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "category_counts = category_rdd.countByValue()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dict is really small (one key-value pair for each category) and will easily fit into memory on the datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78829"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_count = sum(category_counts.values())\n",
    "review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, so is this alternative approach faster or was that just a coincidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain number of ocurrences of each term by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define an RDD for extracting `category` and `reviewText` from each review JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:34:45.585066Z",
     "start_time": "2023-04-29T18:34:45.573062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 971 µs, sys: 0 ns, total: 971 µs\n",
      "Wall time: 844 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "category_review_rdd = (\n",
    "    input_rdd\n",
    "    .map(lambda json_str: json.loads(json_str))\n",
    "    .map(lambda json_obj: (json_obj['category'], json_obj['reviewText']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tokenize the review texts and remove stopwords to obtain the terms. For each unique term appearing in each document, output a tuple of the form $$((term, category), 1)$$ where $term$ is the respective term and $category$ is the category the document is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 2.27 ms, total: 13.7 ms\n",
      "Wall time: 68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define pattern for splitting/tokenizing\n",
    "import re\n",
    "pattern = re.compile(r\"[^a-zA-Z<>^|]+\")\n",
    "\n",
    "def map_review_data(pair):\n",
    "    category, review_text = pair\n",
    "    # obtain set of unique(!) terms for each document via tokenization followed by stopword removal\n",
    "    terms = [\n",
    "        t\n",
    "        for t in set(token.lower() for token in pattern.split(review_text))\n",
    "        if t not in stopwords and len(t) >= 2\n",
    "    ]\n",
    "    return [((term, category), 1) for term in terms]\n",
    "\n",
    "def remap(pair):\n",
    "    term_and_cat, count = pair\n",
    "    term, cat = term_and_cat\n",
    "    return term, (cat, count)\n",
    "\n",
    "term_cat_occ_rdd = (\n",
    "    category_review_rdd.flatMap(\n",
    "        map_review_data\n",
    "    )\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .map(remap) # can I avoid having to do this somehow?\n",
    "    .groupByKey()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data to compute the $\\chi^2$ metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T18:53:14.481513Z",
     "start_time": "2023-04-29T18:53:14.460034Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_chi_square(pair):\n",
    "    term, term_counts_for_categories = pair\n",
    "    term_count_list = [t for t in term_counts_for_categories]\n",
    "\n",
    "    doc_count_for_cat = dict(term_count_list) # use to retrieve no. of documents containing term for a particular category\n",
    "    total_doc_count_for_term = sum(doc_count_for_cat.values()) # total number of documents containing the term\n",
    "    \n",
    "    term_and_cat_chi_squared = []\n",
    "    \n",
    "    for category, count in doc_count_for_cat.items():\n",
    "        a = count # number of documents in c which contain t\n",
    "        b = total_doc_count_for_term - a # number of documents not in c which contain t\n",
    "        total_doc_count_for_cat = category_counts[category] # total no. of documents for current category\n",
    "        c = total_doc_count_for_cat - a # number of documents in c without t\n",
    "        d = review_count - a - b - c # number of documents not in c without t\n",
    "        term_and_cat_chi_squared.append(\n",
    "            (\n",
    "                category,\n",
    "                (term, review_count * (a * d - b * c) ** 2 / ((a + b) * (a + c) * (b + d) * (c + d)))\n",
    "            )\n",
    "        )\n",
    "    return term_and_cat_chi_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Patio_Lawn_and_Garde', ('term', 426.28068327509436)),\n",
       " ('Apps_for_Android', ('term', 33.22447905348442)),\n",
       " ('Book', ('term', 1.246657147592458))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "cat1, cat2, cat3 = [c for c in category_counts.keys()][:3]\n",
    "pair = ('term', [(cat1, 10), (cat2, 5), (cat3, 3)])\n",
    "calculate_chi_square(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.29 ms, sys: 1.29 ms, total: 6.58 ms\n",
      "Wall time: 22.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the chi-squared value for each unique term and category pair\n",
    "# (term, category) -> chi-square\n",
    "term_cat_chi_squared_rdd = (term_cat_occ_rdd\n",
    "    .flatMap(calculate_chi_square)\n",
    "    .groupByKey()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract top 75 terms for each category (sorted by $\\chi^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 ms, sys: 8.57 ms, total: 35.4 ms\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Perform top K query for each category\n",
    "topK = 75  # Number of terms to retrieve per category\n",
    "\n",
    "def get_top_terms(pair):\n",
    "    category, terms = pair\n",
    "    top_terms = sorted(terms, key=lambda x: x[1], reverse=True)[:topK]  # Sort and retrieve top K terms\n",
    "    return category, top_terms\n",
    "\n",
    "# Apply the transformation and collect the results\n",
    "results = term_cat_chi_squared_rdd.map(get_top_terms).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results) == len(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Apps_for_Android',\n",
       " [('games', 3081.1493374842926),\n",
       "  ('play', 2158.3694068201294),\n",
       "  ('graphics', 1505.5108977351497),\n",
       "  ('kindle', 1470.820942569012),\n",
       "  ('addictive', 1311.905562727777),\n",
       "  ('challenging', 1038.1284558527927),\n",
       "  ('coins', 1002.6647889526382),\n",
       "  ('addicting', 990.8441134974868),\n",
       "  ('fire', 956.1470053110605),\n",
       "  ('levels', 825.3813282736016),\n",
       "  ('playing', 692.9340396014182),\n",
       "  ('ads', 642.3969794099202),\n",
       "  ('puzzles', 596.7716753070063),\n",
       "  ('apps', 548.7810653104153),\n",
       "  ('free', 500.9884786241356),\n",
       "  ('bingo', 409.2358492981346),\n",
       "  ('mahjong', 322.00891943980963),\n",
       "  ('download', 303.8649278202287),\n",
       "  ('faotd', 288.8577201586641),\n",
       "  ('facebook', 282.51705437029005),\n",
       "  ('downloaded', 262.77022492215735),\n",
       "  ('hints', 242.61029019440056),\n",
       "  ('solitaire', 211.6429957838186),\n",
       "  ('android', 211.58105849598613),\n",
       "  ('puzzle', 198.85582217352504),\n",
       "  ('gameplay', 198.5123356770461),\n",
       "  ('freezes', 189.67737127837006),\n",
       "  ('unlock', 185.7521008338788),\n",
       "  ('played', 180.39650447458513),\n",
       "  ('deleted', 179.2243589462116),\n",
       "  ('bought', 174.4587211734982),\n",
       "  ('flappy', 173.30583696524425),\n",
       "  ('upgrades', 168.99856742047183),\n",
       "  ('awesome', 155.21008166532062),\n",
       "  ('tablet', 155.13822220891723),\n",
       "  ('price', 149.5959088208227),\n",
       "  ('calculator', 148.95756302858823),\n",
       "  ('developer', 148.37746519215403),\n",
       "  ('quality', 144.19395657092494),\n",
       "  ('permissions', 137.37091512558038),\n",
       "  ('author', 135.34205354169052),\n",
       "  ('earn', 134.7527163155313),\n",
       "  ('bored', 131.23190073884672),\n",
       "  ('uninstall', 127.29106315507559),\n",
       "  ('sudoku', 126.99968483964763),\n",
       "  ('fit', 126.10402357270084),\n",
       "  ('years', 124.35784441658689),\n",
       "  ('reading', 121.6512068956907),\n",
       "  ('made', 120.91456375779856),\n",
       "  ('characters', 118.09788512184657),\n",
       "  ('gameloft', 115.53429315666729),\n",
       "  ('series', 115.05716592780018),\n",
       "  ('written', 114.84060961362133),\n",
       "  ('addicted', 109.65036786251359),\n",
       "  ('crashes', 107.3093981107394),\n",
       "  ('brain', 106.92909693130997),\n",
       "  ('multiplayer', 102.51457697684802),\n",
       "  ('challenge', 102.18967874435549),\n",
       "  ('sims', 100.44868563758436),\n",
       "  ('uninstalling', 100.32193506574053),\n",
       "  ('tetris', 99.24357491450239),\n",
       "  ('back', 99.12491188526246),\n",
       "  ('glitches', 99.02553697390849),\n",
       "  ('end', 95.24822201431749),\n",
       "  ('waster', 94.22529346826073),\n",
       "  ('crashing', 93.68803122186573),\n",
       "  ('downloading', 93.54235192211287),\n",
       "  ('size', 93.34366680110553),\n",
       "  ('put', 91.54071641565497),\n",
       "  ('logos', 88.26571319760414),\n",
       "  ('minecraft', 88.26571319760414),\n",
       "  ('glu', 86.64962060177764),\n",
       "  ('pou', 86.64962060177764),\n",
       "  ('freecell', 86.64962060177764),\n",
       "  ('apos', 86.64962060177764)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0] # check if structure of output is as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks legit!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain all unique tokens from Top 75 for every category (ordered alphabetically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 766 µs, sys: 0 ns, total: 766 µs\n",
      "Wall time: 786 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens = sorted(set(term for _, top75_for_cat in results for term, _ in top75_for_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format of each output line: \"<category> term1:chi_squared1 term2:chi_squared2 ... term75:chi_squared75\"\n",
    "# finally, append the list of tokens to the end of the file\n",
    "with open(f\"output_rdd{'_full_dataset' if review_count > 7000000 else ''}.txt\", \"w\") as file:\n",
    "    for pair in results:\n",
    "        file.write(\"<%s>\" % pair[0] + \" \")\n",
    "        for token, chi_square in pair[1]:\n",
    "            file.write(\"%s:%f\" % (token, chi_square) + \" \")\n",
    "        file.write(\"\\n\")\n",
    "    file.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop() # stop Spark context to free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
