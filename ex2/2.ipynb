{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T14:05:35.598726Z",
     "start_time": "2023-05-09T13:49:17.959564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/05/09 13:49:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/09 13:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/09 13:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/09 13:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/09 13:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/05/09 13:49:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/05/09 13:50:15 WARN DAGScheduler: Broadcasting large task binary with size 1065.6 KiB\n",
      "23/05/09 13:50:16 WARN DAGScheduler: Broadcasting large task binary with size 1067.8 KiB\n",
      "23/05/09 13:50:25 WARN DAGScheduler: Broadcasting large task binary with size 1070.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector, RegexTokenizer, CountVectorizer, StringIndexer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the path to the input file\n",
    "input_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "# input_path = \"hdfs:///user/e11809642/reviews/reduced_devset.json\"\n",
    "\n",
    "# Create or retrieve a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ChiSquaredPipeline\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read the input file as a DataFrame\n",
    "df = spark.read.json(input_path)\n",
    "\n",
    "# Define the regular expression pattern\n",
    "pattern = \"[^a-zA-Z<>^|]+\"\n",
    "\n",
    "# Tokenize the review text using a regex tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=pattern, gaps=True)\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Filter out stopwords from the tokens using a stop words remover\n",
    "stopwords = sc.textFile(\"hdfs:///user/e11809642/stopwords.txt\").collect()\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwords)\n",
    "df = remover.transform(df)\n",
    "\n",
    "# Convert the category column to a numeric type\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Compute the term frequency for each term in each category\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", binary=True)\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "# Create a ChiSqSelector to select the top 2000 features\n",
    "css = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\", numTopFeatures=2000)\n",
    "css_model = css.fit(df)\n",
    "df = css_model.transform(df)\n",
    "\n",
    "selected_features = css_model.selectedFeatures\n",
    "\n",
    "feature_names = [cv_model.vocabulary[i] for i in selected_features]\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, cv, css])\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
