{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Datasets/DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector, RegexTokenizer, StringIndexer, IDF, StopWordsRemover, \\\n",
    "    Normalizer, CountVectorizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a SparkConf with the following settings:\n",
    "# - spark.driver.memory: 4g\n",
    "# - spark.executor.memory: 7392m\n",
    "# - spark.parallelism: 4\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\\n",
    "    .set(\"spark.executor.memory\", \"7392m\") \\\n",
    "    .set(\"spark.parallelism\", \"4\") \\\n",
    " \\\n",
    "    # Create a SparkSession with the name \"ChiSquaredPipeline\" using the SparkConf above\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChiSquaredPipeline\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Retrieve the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set the log level to WARN\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Read the review file into a DataFrame\n",
    "# review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "df = spark.read.json(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the stopword file as a set from the local file system\n",
    "stopwords = set(open(\"stopwords.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the review text into words using a regular expression pattern\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"[^a-zA-Z<>^|]+\", gaps=True,\n",
    "                           toLowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from the tokenized words list using the stopword set\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Convert the category column to a numeric type using the StringIndexer with alphabetically ascending order to allow for easy mapping to the category names later\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\", stringOrderType=\"alphabetAsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the term frequency vector for each document (review)\n",
    "# tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "tf = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the inverse document frequency vector for each document (review)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Select the top 2000 features based on the chi-squared test for feature independence\n",
    "css = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\", numTopFeatures=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline combining all steps\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, tf, idf, css])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the DataFrame\n",
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:26.844445700Z",
     "start_time": "2023-05-30T12:46:26.361445800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary and selected features\n",
    "vocab = pipeline_model.stages[3].vocabulary\n",
    "selected_features = pipeline_model.stages[5].selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:26.857490100Z",
     "start_time": "2023-05-30T12:46:26.850664700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the names of the selected features to a file sorted alphabetically in ascending order (space separated)\n",
    "with open(\"selected_features.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(sorted([vocab[i] for i in selected_features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:26.892444800Z",
     "start_time": "2023-05-30T12:46:26.859477900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training, test, and validation sets\n",
    "training_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.726532600Z",
     "start_time": "2023-05-30T12:46:26.893441800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of category names sorted alphabetically in ascending order\n",
    "category_names = sorted([row[\"category\"] for row in df.select(\"category\").distinct().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.732548100Z",
     "start_time": "2023-05-30T12:46:27.728541900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of categories\n",
    "num_classes = len(category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.751146400Z",
     "start_time": "2023-05-30T12:46:27.733999900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize each Vector using $L^2$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.789322Z",
     "start_time": "2023-05-30T12:46:27.777322100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an SVM classifier using the normalized features and the category index\n",
    "svm = LinearSVC(featuresCol=\"normFeatures\", labelCol=\"categoryIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.797323900Z",
     "start_time": "2023-05-30T12:46:27.791351500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a one-vs-rest classifier using the SVM classifier\n",
    "ovr = OneVsRest(classifier=svm, featuresCol=\"normFeatures\", labelCol=\"categoryIndex\", parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.804327500Z",
     "start_time": "2023-05-30T12:46:27.800319300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline combining all steps\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, tf, idf, normalizer, css, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:46:27.835319900Z",
     "start_time": "2023-05-30T12:46:27.805349700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an evaluator for the F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-30T12:46:27.836332400Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the micro average F1 score using the test data\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"Micro-Average F1 score: %f\" % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a parameter grid for the SVM classifier with the following parameters:\n",
    "# - numTopFeatures: 50, 2000\n",
    "# - regParam: 0.1, 0.01, 0.001\n",
    "# - standardization: True, False\n",
    "# - maxIter: 10, 100\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(css.numTopFeatures, [50, 2000]) \\\n",
    "    .addGrid(svm.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(svm.standardization, [True, False]) \\\n",
    "    .addGrid(svm.maxIter, [10, 100]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a train-validation-split using the pipeline, parameter grid, and evaluator\n",
    "tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, trainRatio=0.8,\n",
    "                           seed=42, parallelism=8, collectSubModels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the train-validation-split to the training data\n",
    "tvs_model = tvs.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write the entire fitted tvs_model to disk\n",
    "tvs_model.write().overwrite().save(\"tvs_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = tvs_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the F1 score for the test data\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the parameters that were selected by the train-validation-split for the best model\n",
    "# for param in param_grid:\n",
    "#     print(param.name, \":\", tvs_model.bestModel.getOrDefault(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a cross-validator using the pipeline, parameter grid, and evaluator\n",
    "# cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "# cv_model = cv.fit(training_data)\n",
    "\n",
    "# Check the parameters that were selected by the cross-validator for the best model\n",
    "# for param in param_grid:\n",
    "#     print(param.name, \":\", cv_model.bestModel.getOrDefault(param))\n",
    "\n",
    "# Make predictions on the test data\n",
    "# predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Compute the F1 score for the test data\n",
    "# f1_score = evaluator.evaluate(predictions)\n",
    "# f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "# pipeline_model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# Compute the micro average F1 score using the test data\n",
    "# f1_score = evaluator.evaluate(predictions)\n",
    "# print(\"Micro-Average F1 score: %f\" % f1_score)\n",
    "\n",
    "# Calculate multiclass metrics for the test data\n",
    "# metrics = MulticlassMetrics(predictions.select(\"prediction\", \"categoryIndex\").rdd)\n",
    "\n",
    "# Select the confusion matrix from the metrics object\n",
    "# confusion_matrix = metrics.confusionMatrix()\n",
    "\n",
    "# Convert the confusion matrix to a Pandas DataFrame for better visualization mapping the category indices to the category names\n",
    "# df = pd.DataFrame(confusion_matrix.toArray(), index=category_names, columns=category_names)\n",
    "# df\n",
    "\n",
    "# Print the f1-score for each category using the metrics object\n",
    "# test = 0\n",
    "# for i in range(num_classes):\n",
    "#     f_score = metrics.fMeasure(float(i))\n",
    "#     test += f_score\n",
    "#     print(\"F1 score for %s: %f\" % (category_names[i], f_score))\n",
    "\n",
    "# Print the macro-average f1-score using the test data\n",
    "# print(\"Macro-Average F1 score: %f\" % (test / num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
