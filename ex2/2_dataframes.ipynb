{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Datasets/DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector, RegexTokenizer, StringIndexer, IDF, StopWordsRemover, \\\n",
    "    Normalizer, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create or retrieve a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ChiSquaredPipeline\").getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the review file into a DataFrame\n",
    "# review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "review_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "df = spark.read.json(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the stopword file as a set from the local file system\n",
    "stopwords = set(open(\"stopwords.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review text into words using a regular expression pattern\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"[^a-zA-Z<>^|]+\", gaps=True,\n",
    "                           toLowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from the tokenized words list using the stopword set\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the category column to a numeric type using the StringIndexer with alphabetically ascending order to allow for easy mapping to the category names later\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\", stringOrderType=\"alphabetAsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the term frequency vector for each document (review)\n",
    "# tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "tf = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the inverse document frequency vector for each document (review)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 2000 features based on the chi-squared test for feature independence\n",
    "css = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\", numTopFeatures=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pipeline combining all steps\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, tf, idf, css])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the DataFrame\n",
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary and selected features\n",
    "vocab = pipeline_model.stages[3].vocabulary\n",
    "selected_features = pipeline_model.stages[5].selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the names of the selected features to a file sorted alphabetically in ascending order (space separated)\n",
    "with open(\"selected_features.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(sorted([vocab[i] for i in selected_features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into training, test, and validation sets\n",
    "training_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of category names sorted alphabetically in ascending order\n",
    "category_names = sorted([row[\"category\"] for row in df.select(\"category\").distinct().collect()])\n",
    "\n",
    "# Get the number of categories\n",
    "num_classes = len(category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each Vector using $L^2$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an SVM classifier using the normalized features and the category index\n",
    "svm = LinearSVC(featuresCol=\"normFeatures\", labelCol=\"categoryIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a one-vs-rest classifier using the SVM classifier\n",
    "ovr = OneVsRest(classifier=svm, featuresCol=\"normFeatures\", labelCol=\"categoryIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pipeline combining all steps\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, tf, idf, normalizer, css, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an evaluator for the F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the micro average F1 score using the test data\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"Micro-Average F1 score: %f\" % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate multiclass metrics for the test data\n",
    "metrics = MulticlassMetrics(predictions.select(\"prediction\", \"categoryIndex\").rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the confusion matrix from the metrics object\n",
    "confusion_matrix = metrics.confusionMatrix()\n",
    "\n",
    "# Convert the confusion matrix to a Pandas DataFrame for better visualization mapping the category indices to the category names\n",
    "df = pd.DataFrame(confusion_matrix.toArray(), index=category_names, columns=category_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the f1-score for each category using the metrics object\n",
    "test = 0\n",
    "for i in range(num_classes):\n",
    "    f_score = metrics.fMeasure(float(i))\n",
    "    test += f_score\n",
    "    print(\"F1 score for %s: %f\" % (category_names[i], f_score))\n",
    "# Print the macro-average f1-score using the test data\n",
    "print(\"Macro-Average F1 score: %f\" % (test / num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a parameter grid for the SVM classifier with the following parameters:\n",
    "# - numTopFeatures: 50, 2000\n",
    "# - regParam: 0.1, 0.01, 0.001\n",
    "# - standardization: True, False\n",
    "# - maxIter: 10, 100\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(css.numTopFeatures, [50, 2000]) \\\n",
    "    .addGrid(svm.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(svm.standardization, [True, False]) \\\n",
    "    .addGrid(svm.maxIter, [10, 100]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a cross-validator using the pipeline, parameter grid, and evaluator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Fit the cross-validator to the training data\n",
    "cv_model = cv.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Check the parameters that were selected by the cross-validator for the best model\n",
    "for param in paramGrid:\n",
    "    print(param.name, \":\", cv_model.bestModel.getOrDefault(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Compute the F1 score for the test data\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Create a train-validation-split using the pipeline, parameter grid, and evaluator\n",
    "tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Fit the train-validation-split to the training data\n",
    "tvs_model = tvs.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = tvs_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Compute the F1 score for the test data\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
