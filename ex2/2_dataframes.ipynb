{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Datasets/DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector, RegexTokenizer, CountVectorizer, StringIndexer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "# input_path = \"hdfs:///user/e11809642/reviews/reduced_devset.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/05/24 18:03:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/05/24 18:03:23 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/05/24 18:03:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ChiSquaredPipeline\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_df = spark.read.json(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[^a-zA-Z<>^|]+\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=pattern, gaps=True)\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopword removal (using local `stopwords.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = sc.textFile(\"hdfs:///user/e11717659/stopwords.txt\").collect()\n",
    "remover = StsopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwords)\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = df.randomSplit([0.7, 0.2, 0.1], seed=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 18:04:14 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|            category|\n",
      "+--------------------+--------------------+\n",
      "|(262144,[7527,758...|Patio_Lawn_and_Garde|\n",
      "|(262144,[1968,232...|Patio_Lawn_and_Garde|\n",
      "|(262144,[29241,29...|Patio_Lawn_and_Garde|\n",
      "|(262144,[8804,230...|Patio_Lawn_and_Garde|\n",
      "|(262144,[1797,243...|Patio_Lawn_and_Garde|\n",
      "|(262144,[2325,892...|Patio_Lawn_and_Garde|\n",
      "|(262144,[10631,17...|Patio_Lawn_and_Garde|\n",
      "|(262144,[3613,150...|Patio_Lawn_and_Garde|\n",
      "|(262144,[5030,196...|Patio_Lawn_and_Garde|\n",
      "|(262144,[22716,33...|Patio_Lawn_and_Garde|\n",
      "|(262144,[16928,39...|Patio_Lawn_and_Garde|\n",
      "|(262144,[1546,389...|Patio_Lawn_and_Garde|\n",
      "|(262144,[13790,19...|Patio_Lawn_and_Garde|\n",
      "|(262144,[7150,310...|Patio_Lawn_and_Garde|\n",
      "|(262144,[2701,635...|Patio_Lawn_and_Garde|\n",
      "|(262144,[991,2325...|Patio_Lawn_and_Garde|\n",
      "|(262144,[3121,578...|Patio_Lawn_and_Garde|\n",
      "|(262144,[121517,1...|Patio_Lawn_and_Garde|\n",
      "|(262144,[3898,223...|Patio_Lawn_and_Garde|\n",
      "|(262144,[140904,1...|Patio_Lawn_and_Garde|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the category column to a numeric type\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "df.select(\"features\", \"category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection w/ Chi Square (top 2000 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 18:04:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/05/24 18:04:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/05/24 18:04:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "css = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\", numTopFeatures=2000)\n",
    "css_model = css.fit(df)\n",
    "df = css_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = css_model.selectedFeatures\n",
    "feature_names = [df.select(\"features\").features[i] for i in selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline combining all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SomeTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m training_data, test_data, validation_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m69\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Add a new stage to the pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m new_stage \u001b[38;5;241m=\u001b[39m \u001b[43mSomeTransformer\u001b[49m(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m updated_stages \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mgetStages() \u001b[38;5;241m+\u001b[39m [new_stage]\n\u001b[1;32m      6\u001b[0m pipeline\u001b[38;5;241m.\u001b[39msetStages(updated_stages)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SomeTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "df = normalizer.transform(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, indexer, hashing_tf, idf, css, normalizer])\n",
    "pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
